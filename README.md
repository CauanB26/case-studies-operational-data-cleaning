[PT-BR] Para a vers√£o em portugu√™s deste documento, role at√© o final da p√°gina.  

---

### **English Version**

> **‚ö†Ô∏è Note:** This is a case study of a data processing pipeline. For confidentiality reasons, other files are not shared.  
 

---
![Python](https://img.shields.io/badge/Python-3.x-blue.svg?style=for-the-badge&logo=python)
![Pandas](https://img.shields.io/badge/Pandas-Data%20Pipeline-green.svg?style=for-the-badge&logo=pandas)
![NumPy](https://img.shields.io/badge/NumPy-Numerical%20Computing-blueviolet.svg?style=for-the-badge&logo=numpy)
![SciPy](https://img.shields.io/badge/SciPy-Statistics%20%26%20Analysis-orange.svg?style=for-the-badge&logo=scipy)
![Matplotlib](https://img.shields.io/badge/Matplotlib-Plotting%20%26%20Visualization-white.svg?style=for-the-badge&logo=matplotlib)
![Data Engineering](https://img.shields.io/badge/Data%20Engineering-ETL%20Pipeline-blueviolet.svg?style=for-the-badge)
![Algorithm](https://img.shields.io/badge/Algorithm-Anomaly%20Detection-9cf.svg?style=for-the-badge)
---

### üéØ Project Objective

The objective of this project was to develop a robust data analysis pipeline to automate data processing and interpretation. The solution transforms raw data extracted from Excel spreadsheets into a clean and reliable series, allowing for more accurate calculations and evaluations, thereby assisting in decision-making regarding maintenance and operation.

---

### üìà The Scenario: A Daily Operational Challenge

Manual data analysis is a complex and imprecise process. Raw data frequently contains anomalies such as:

* **Invalid Values:** Readings outside a realistic spectrum.  
* **Noise and Spikes:** Momentary fluctuations that do not represent the actual trend.  
* **Data Gaps:** Periods with reading failures or interrupted communication.  
* **Reset Events:** Sudden drops in values that "reset" the cycle and need to be handled differently.  

Manually analyzing this data is time-consuming and subjective, making it difficult to obtain a reliable daily indicator to plan operational actions.

---

### ‚ú® The Solution: A Python Analysis Pipeline

To solve this problem, I developed a Python script that implements a sequential and highly specialized data processing pipeline. The tool ingests the data, applies a series of statistical and heuristic corrections, and generates a final, actionable report. The technical features include:

**‚úÖ Data Correction Pipeline:**
* Executes a sequence of 8 logical and configurable steps, where the output of one step serves as the input for the next, ensuring consistent and auditable data treatment.

**‚úÖ Statistical Outlier Treatment:**
* Uses the trimmed mean (`scipy.stats.trim_mean`) to calculate the daily aggregated value, removing the 20% most extreme data points each day to mitigate the effect of anomalous readings.

**‚úÖ Anomaly Detection Algorithms:**
* Implements custom logic to identify and correct data spikes that are not sustained over time, as well as small drops (noise) that do not correspond to actual system events.

**‚úÖ Normalization by Performance Cycles:**
* The most complex stage of the pipeline: the script automatically identifies "reset events" and uses these points to segment the time series into "cycles." Within each cycle, it reconstructs a linear ramp from the beginning of the cycle to its peak, creating an idealized and clean model of the trend.

---

### üî¨ Visual Validation of the Pipeline

The graphs below are a direct output from the script's debug mode, visually demonstrating the effectiveness of the data treatment algorithms at different stages.

![Graph 1: Spike and Noise Treatment](imgs/grafico1.png)
*<p align="center">Graph 1</p>*

![Graph 2: Correction of Data Gaps](imgs/grafico2.png)
*<p align="center">Graph 2</p>*

![Graph 3: Normalization by Cycles](imgs/grafico3.png)
*<p align="center">Graph 3</p>*

These graphs show the "before" (dashed line) and "after" (solid line) of the data at each critical phase, providing tangible proof of the pipeline's value in transforming raw, noisy data into a clean and actionable time series.

---

### üöÄ Results and Impact

The implementation of this automated pipeline generated a leap in quality and efficiency in data analysis.

| Area             | Before Automation                        | After Automation                             |
| :--------------- | :--------------------------------------- | :------------------------------------------- |
| **Data Quality** | Raw data, with noise and gaps            | Clean, corrected, and normalized time series |
| **Analysis** | Subjective and based on visual inspection | Objective and based on statistical algorithms |
| **Reliability** | Low confidence in the daily performance indicator | Calculation of a robust and defensible daily indicator |
| **Analysis Time**| Hours of manual work per analysis        | Seconds of script execution                  |

The main result was the ability to quickly and reliably generate an indicator, allowing for better operational planning.

---

### üí° Skills and Competencies Demonstrated

* **Data Engineering:** Design and implementation of an ETL (Extract, Transform, Load) and data processing pipeline.  
* **Statistical Data Analysis:** Advanced use of `pandas`, `numpy`, and `scipy` libraries for cleaning, transformation, interpolation, and statistical analysis.  
* **Algorithm Development:** Creation of custom heuristics and algorithms for anomaly detection and time series normalization.  
* **Python Development:** Structuring a modular, parameterized, and well-practiced script for maintainability.  

---
---
---


### **Vers√£o em Portugu√™s**

> **‚ö†Ô∏è Observa√ß√£o:** Este √© um case study de um pipeline de processamento de dados. Por motivos de confidencialidade, os demais arquivos n√£o s√£o compartilhados.

---
![Python](https://img.shields.io/badge/Python-3.x-blue.svg?style=for-the-badge&logo=python)
![Pandas](https://img.shields.io/badge/Pandas-Pipeline%20de%20Dados-green.svg?style=for-the-badge&logo=pandas)
![NumPy](https://img.shields.io/badge/NumPy-Computa√ß√£o%20Num√©rica-blueviolet.svg?style=for-the-badge&logo=numpy)
![SciPy](https://img.shields.io/badge/SciPy-Estat√≠stica%20%26%20An√°lise-orange.svg?style=for-the-badge&logo=scipy)
![Matplotlib](https://img.shields.io/badge/Matplotlib-Gera√ß√£o%20de%20Gr√°ficos-white.svg?style=for-the-badge&logo=matplotlib)
![Engenharia de Dados](https://img.shields.io/badge/Engenharia%20de%20Dados-Pipeline%20ETL-blueviolet.svg?style=for-the-badge)
![Algoritmo](https://img.shields.io/badge/Algoritmo-Detec√ß√£o%20de%20Anomalias-9cf.svg?style=for-the-badge)
---
### üéØ Objetivo do Projeto

O objetivo deste projeto foi desenvolver um pipeline de an√°lise de dados robusto para automatizar o processamento e a interpreta√ß√£o de dados. A solu√ß√£o trata dados brutos extra√≠dos de planilhas Excel, em uma s√©rie limpa e confi√°vel, permitindo o c√°lculos e avalia√ß√µes mais precisas, auxiliando na tomada de decis√µes sobre manuten√ß√£o e opera√ß√£o.

---

### üìà O Cen√°rio: Um Desafio Operacional Di√°rio

A an√°lise manual de dados √© um processo complexo e impreciso. Os dados brutos frequentemente cont√™m anomalias como:

* **Valores Inv√°lidos:** Leituras fora de um espectro realista.  
* **Ru√≠do e Picos:** Flutua√ß√µes moment√¢neas que n√£o representam a tend√™ncia real.  
* **Lacunas de Dados:** Per√≠odos com falha de leituras ou comunica√ß√£o interrompida.  
* **Eventos de Reset:** Quedas bruscas nos valores que "resetam" o ciclo e precisam ser tratadas de forma distinta.  

A an√°lise manual desses dados √© demorada e subjetiva, dificultando a obten√ß√£o de um indicador di√°rio confi√°vel para planejar as a√ß√µes operacionais.

---

### ‚ú® A Solu√ß√£o: Um Pipeline de An√°lise em Python

Para resolver este problema, desenvolvi um script em Python que implementa um pipeline de processamento de dados sequencial e altamente especializado. A ferramenta ingere os dados, aplica uma s√©rie de corre√ß√µes estat√≠sticas e heur√≠sticas, e gera um relat√≥rio final acion√°vel. Os recursos t√©cnicos incluem:

**‚úÖ Pipeline de Corre√ß√£o de Dados:**
* Executa uma sequ√™ncia de 8 etapas l√≥gicas e configur√°veis, onde a sa√≠da de uma etapa serve como entrada para a pr√≥xima, garantindo um tratamento de dados consistente e audit√°vel.

**‚úÖ Tratamento Estat√≠stico de Outliers:**
* Utiliza a m√©dia aparada (`scipy.stats.trim_mean`) para calcular o valor di√°rio agregado, removendo os 20% de dados mais extremos de cada dia para mitigar o efeito de leituras an√¥malas.

**‚úÖ Algoritmos de Detec√ß√£o de Anomalias:**
* Implementa l√≥gicas customizadas para identificar e corrigir picos de dados que n√£o se sustentam ao longo do tempo, bem como pequenas quedas (ru√≠dos) que n√£o correspondem a eventos de sistema reais.

**‚úÖ Normaliza√ß√£o por Ciclos de Performance:**
* A etapa mais complexa do pipeline: o script identifica automaticamente "eventos de reset" e usa esses pontos para segmentar a s√©rie temporal em "ciclos". Dentro de cada ciclo, ele reconstr√≥i uma rampa linear, do in√≠cio do ciclo at√© seu pico, criando um modelo idealizado e limpo da tend√™ncia.

---

### üî¨ Valida√ß√£o Visual do Pipeline

Os gr√°ficos abaixo s√£o uma sa√≠da direta do modo de depura√ß√£o do script, demonstrando visualmente a efic√°cia dos algoritmos de tratamento de dados em diferentes fases.

![Gr√°fico 1: Tratamento de Picos e Ru√≠dos](imgs/grafico1.png)
*<p align="center">Gr√°fico 1</p>*

![Gr√°fico 2: Corre√ß√£o de Lacunas de Dados](imgs/grafico2.png)
*<p align="center">Gr√°fico 2</p>*

![Gr√°fico 3: Normaliza√ß√£o por Ciclos](imgs/grafico3.png)
*<p align="center">Gr√°fico 3</p>*

Estes gr√°ficos mostram o "antes" (linha tracejada) e o "depois" (linha s√≥lida) dos dados em cada fase cr√≠tica, fornecendo uma prova tang√≠vel do valor do pipeline ao transformar dados brutos e ruidosos em uma s√©rie temporal limpa e acion√°vel.

---

### üöÄ Resultados e Impacto

A implementa√ß√£o deste pipeline automatizado gerou um salto de qualidade e efici√™ncia na an√°lise dos dados.

| √Årea                  | Antes da Automa√ß√£o                       | Depois da Automa√ß√£o                            |
| :-------------------- | :--------------------------------------- | :--------------------------------------------- |
| **Qualidade dos Dados** | Dados brutos, com ru√≠dos e lacunas       | S√©rie temporal limpa, corrigida e normalizada  |
| **An√°lise** | Subjetiva e baseada em inspe√ß√£o visual   | Objetiva e baseada em algoritmos estat√≠sticos  |
| **Confiabilidade** | Baixa confian√ßa no indicador de performance di√°rio | C√°lculo de um indicador di√°rio robusto e defens√°vel |
| **Tempo de An√°lise** | Horas de trabalho manual por an√°lise     | Segundos de execu√ß√£o do script                 |

O principal resultado foi a capacidade de gerar, de forma r√°pida e confi√°vel, um indicador, permitindo um melhor planejamento operacional.

---

### üí° Habilidades e Compet√™ncias Demonstradas

* **Engenharia de Dados:** Concep√ß√£o e implementa√ß√£o de um pipeline de ETL (Extra√ß√£o, Transforma√ß√£o e Carga) e processamento de dados.  
* **An√°lise Estat√≠stica de Dados:** Uso avan√ßado das bibliotecas `pandas`, `numpy` e `scipy` para limpeza, transforma√ß√£o, interpola√ß√£o e an√°lise estat√≠stica.  
* **Desenvolvimento de Algoritmos:** Cria√ß√£o de heur√≠sticas e algoritmos customizados para detec√ß√£o de anomalias e normaliza√ß√£o de s√©ries temporais.  
* **Desenvolvimento em Python:** Estrutura√ß√£o de um script modular, parametrizado e com boas pr√°ticas para manuten√ß√£o.  
